---
title: "Chapter 2 homework"
format:
  html:
    toc: true
    toc-depth: 3
embed-resources: true
execute:
  echo: true
  message: false
  warning: false
---

## Setup

Assume you are working in an RStudio Project. Use `here::here("data", "filename.csv")` to build file paths to datasets in the `data` folder.

```{r}
library(tidyverse)
library(here)
library(psych)   # used for describe()/describeBy() in a few places
```

```{r}
heart <- read_csv(here::here("data", "heart.csv"))
```

::: callout-tip
If you ever get a “file not found” error, confirm (1) you opened the correct `.Rproj`, and (2) the dataset is inside your project’s data folder (accessed via `here::here("data", ...)`).
:::

## Tutorial (Chapter 2, plus a bit of 3)

### 1) Estimates of variation around the mean

In class, we talked about quantifying variation. Variance and standard deviation are both based on the **sum of squared error (SSE)** around a model’s predictions.

Start by computing the sample standard deviation and sample variance of height:

```{r}
#format data
d <- heart |>
  rename_with(tolower)

# Compute sd and variance of height
d <- d |> 
  drop_na(height) |>
  mutate(height_mean = mean(height),
         height_err = height - height_mean)

sse_height <- sum(d$height_err^2)
sse_height

var_height <- (sse_height)/(nrow(d) - 1)
sd_height <- sqrt(var_height)

sd_height == sd(d$height)


```

#### Convert variance to standard deviation

Recall: $\text{SD} = \sqrt{\text{Var}}$.

```{r}
# Convert variance to SD using R as a calculator
a <- sd_height^2
a <- round(a, 10)
b <- round(var_height, 10)
a == b
```

#### Compute SSE from the variance and sample size

For the *sample* variance, $$
s^2 = \frac{\text{SSE}}{n-1}
\quad\Rightarrow\quad
\text{SSE} = s^2 (n-1).
$$

```{r}
# Compute n, variance, and then SSE = var * (n - 1)

n <- d |>
  drop_na(height) |>
  summarize(count = n())

var_height
SSE <- (n - 1) * var_height
SSE == sse_height
```

::: callout-tip
In R, `var(x)` uses the sample variance with denominator $n-1$. That’s why the formula above uses $n-1$.
:::

### 2) Estimates of central tendency

Compute mean and median:

```{r}
# Mean and median of height
height_sum <- d |>
  drop_na(height) |>
  summarize(mean_height = mean(height),
            median_height = median(height))

height_sum
```

#### Mode (custom helper)

R’s base `mode()` is **not** the statistical mode. Below is a simple helper that returns the most frequent value(s). If the data are multimodal, it returns the average of the modes.

```{r}
mymode <- function(x) {
  x2 <- na.omit(x)
  ux <- unique(x2)
  tab <- tabulate(match(x2, ux))
  mean(ux[tab == max(tab)])
}
```

```{r}
# Use mymode() to compute the mode of height
mode_height <- mymode(d$height)
```

### 3) Group and summarize a dataset

Compute mean and SD of height by gender using tidyverse verbs:

```{r}
# Summarize Height by Gender:
# - n
# - mean height
# - sd height

sum_bygender <- d |>
  drop_na(height, gender) |>
  group_by(gender) |> 
  summarize(count = n(),
            mean_height = mean(height),
            sd_height = sd(height))

sum_bygender
```

If you want a richer descriptive table, `psych::describeBy()` can do that:

```{r}
describeBy(d$height, d$gender, mat = TRUE)
```

::: callout-tip
Even when you use a helper like `describeBy()`, prefer doing **wrangling** (filtering, selecting, mutating, grouping) with tidyverse verbs first, then pass the result to the summary function.
:::

### 4) Estimates of variation revisited: SSE from the data

One way to compute SSE around the **mean model** (predict $\bar{Y}$ for everyone):

```{r}
# Compute SSE around the mean
d <- d |> 
  drop_na(height) |>
  mutate(height_mean = mean(height),
         height_err_mean = height - height_mean)

sse_height_mean <- sum(d$height_err_mean^2)
sse_height_mean
```

Now modify that idea to compute SSE around the **median model** (predict $\tilde{Y}$ for everyone).

```{r}
# Compute SSE around the median
d <- d |> 
  drop_na(height) |>
  mutate(height_median = median(height),
         height_err_median = height - height_median)

sse_height_median <- sum(d$height_err_median^2)
sse_height_median
```

Answer in words: is SSE around the median larger or smaller than SSE around the mean for these data?

The SSE around the median is larger that the SSE around the mean.

### 5) Missing data

Many functions accept `na.rm = TRUE` to ignore missing values.

```{r}
# Example: mean height ignoring missing values
```

You can also drop missing values explicitly:

```{r}
heart |> drop_na(Height) |> summarize(mean_height = mean(Height))
```

## HW 02 Questions

### 1) Central tendency and variability (USNEWS)

The dataset `USNEWS.csv` contains data used by *U.S. News and World Report* to make its college rankings. Two variables of interest are:

-   `gradRate`: graduation rate (percent from 0 to 100)
-   `accptRate`: acceptance rate (proportion from 0 to 1)

For each variable, obtain estimates of central tendency (mean, median, mode) and variability (variance and standard deviation). Write a few sentences describing what you found.

The mean of the graduation rate is relatively close to the median. There is a large amount of standard deviation in the graduation rate as the deviation is 18.8. The nonmissing count for graduation rate and acceptance rate is the same meaning there are no records missing only graduation rate or acceptance rate. The mean of the acceptance rate is relatively farther from the median. There is a large amount of standard deviation in the acceptance rate as well as the deviation is 0.161.

```{r}
usnews <- read_csv(here::here("data", "USNEWS.csv"))
```

```{r}
# Create a compact summary table for gradRate and accptRate:
# mean, median, mode, variance, sd, and n (non-missing)
usnews <- usnews |>
  mutate(
    gradRate = as.numeric(gradRate),
    accptRate = as.numeric(accptRate)
  )

gradRate_summary <- usnews |>
  drop_na(gradRate) |>
  summarize(mean_gradRate = mean(gradRate),
            median_gradRate = median(gradRate),
            mode_gradRate = mymode(gradRate),
            var_gradRate = var(gradRate),
            sd_gradRate = sd(gradRate),
            count = n())

accptRate_summary <- usnews |>
  drop_na(accptRate) |>
  summarize(mean_accptRate = mean(accptRate),
            median_accptRate = median(accptRate),
            mode_accptRate = mymode(accptRate),
            var_accptRate = var(accptRate),
            sd_accptRate = sd(accptRate),
            count = n())

gradRate_summary
accptRate_summary
```

### 2) Conditional vs unconditional predictions (USNEWS)

Another interesting variable is `Type` (public vs private).

1.  Write **Model C** that makes a constant prediction for every school.

```{r}
mod_c <- lm(gradRate ~ 1,
            data = usnews)

tidy(mod_c)
```

2.  Write **Model A** that makes predictions of `gradRate` conditional on `Type`.

```{r}
usnews <- usnews |> 
  mutate(Private = if_else(Type == "Private", 1, 0))

mod_a <- lm(gradRate ~ 1 + Private,
             data = usnews)

tidy(mod_a)
```

3.  As a first look at whether Model A might be useful, compute the mean `gradRate` for private and public schools.
4.  Write a sentence or two describing these results and whether it *appears* useful to move from Model C to Model A.

Model A appears more useful than Model C. The graduation rate for private schools is 16% higher compared to public schools. This large difference means control for the type of school would likely reduce error in the model.

Use both a verbal description and a model statement in LaTeX.

#### Model statements (fill in)

-   Model C (compact): $$
    gradRate_i = b_0 + \varepsilon_i
    $$
-   Model A (augmented; conditional means by type): $$
    gradRate_i = b_0 + b_1 X_i + \varepsilon_i
    $$ where $X_i$ is an indicator you define (e.g., $X_i = 1$ if Private, $0$ if Public).

```{r}
# Compute mean gradRate by Type (and include n)
sum_byType <- usnews |>
  drop_na(gradRate, Type) |>
  group_by(Type) |> 
  summarize(count = n(),
            mean_gradRate = mean(gradRate))
sum_byType
```

### 3) Coupon campaign and store sales (stores)

At 10 grocery stores, a market researcher records the number of cases of a product sold both before and after coupons were mailed to households in the area. The data below are the **changes** in the number of cases sold (positive = more after coupons, negative = fewer, zero = no change).

| Store | Change |
|------:|-------:|
|     A |      5 |
|     B |      4 |
|     C |     -2 |
|     D |      6 |
|     E |      1 |
|     F |      0 |
|     G |     -4 |
|     H |      3 |
|     I |      2 |
|     J |      7 |

#### 3a) By hand

Calculate the **mean** and **standard deviation** for the change scores by hand (or in Excel).

(Optional check in R after you finish your by-hand work:)

```{r}
# Create a vector of the change scores and compute mean/sd as a check
```

#### 3b) Model C (no change)

Specify a Model C that predicts **no change** for every store.

Write it in the same form as class:

$$
Y_i = \text{(prediction)} + \varepsilon_i
$$

```{r}
# Write Model C in words (in the text), then compute SSE(C) in R later
```

#### 3c) Model A (best constant prediction from the data)

Specify a Model A with one parameter that makes the best possible constant prediction of `Change` based on the data.

$$
Y_i = b_0 + \varepsilon_i
$$

```{r}
# Write Model A in words (in the text), then compute SSE(A) in R later
```

#### 3d) Null hypothesis and interpretation

State the null hypothesis tested by comparing Model C and Model A, and give a non-technical interpretation of what the comparison asks.

#### 3e) Compute error for both models (SSE)

Using SSE as your aggregate measure of error, compute:

-   $\text{SSE}(C)$ for the compact model
-   $\text{SSE}(A)$ for the augmented model

```{r}
# Compute SSE(C) and SSE(A)
```

#### 3f) Proportional reduction in error (PRE)

Compute: $$
\text{PRE} = \frac{\text{SSE}(C) - \text{SSE}(A)}{\text{SSE}(C)}.
$$

Based on this PRE, do you think Model C should be rejected in favor of Model A? (No formal test yet—explain your reasoning.)

```{r}
# Compute PRE from SSE(C) and SSE(A)
```

#### 3g) Do it again using `stores.csv`

These data are also available in `stores.csv`. Read in the data and use R to obtain all the numbers you calculated above (including PRE if you can).

```{r}
stores <- read_csv(here::here("data", "stores.csv"))
```

```{r}
# Use the stores data to compute:
# - mean and sd of Change
# - SSE(C) where prediction is 0
# - SSE(A) where prediction is mean(Change)
# - PRE
```

::: callout-tip
A convenient SSE pattern is:

-   If predictions are stored in `y_hat`, then `sum((y - y_hat)^2)`.
-   For Model C here, `y_hat` is a constant 0.
-   For Model A here, `y_hat` is a constant equal to `mean(y)`.
:::

### 4) Concept questions

In your own words (**not** using the example from class):

1.  Why is $\text{ERROR}(\text{Model A}) \le \text{ERROR}(\text{Model C})$?
2.  What is a **degree of freedom**?

### 5) Sampling distributions and small samples

Visit the app:

-   https://correll.shinyapps.io/centralTendency/

5a) Generate four or five iterations using a normally distributed population and samples of $n = 5$.

-   Where do the sampling distributions of the mean peak?
-   How much do the means and medians vary? What minimum and maximum values do you see for each?

5b) Answer the same questions for the mean using $n = 500$. (Rescale the histograms so you can see variability.)

### 6) Central Limit Theorem intuition

Visit the app:

-   https://correll.shinyapps.io/centralLimit/

6a) Set a **skewed** population and draw samples of different sizes. What is the shape of the sampling distribution of the mean for samples of $n = 2$?

6b) What is the shape for $n = 10$?

6c) What is the shape for $n = 100$?
